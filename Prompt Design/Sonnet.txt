Sonnet:

I've read through this entire comprehensive analysis. This is excellent diagnostic work - Perplexity has identified the core architectural conflicts very clearly.
What I see:
CRITICAL ISSUE #1: Scale Conflict

buildBasePrompt() tells LLMs to use A/B/C/D/E letter grades
Every single addendum + system prompt tells LLMs to use 0-100 numeric scores
Result: Every LLM receives 2-4 layers of conflicting instructions

CRITICAL ISSUE #2: JSON Key Name Conflict

buildBasePrompt() expects: city1Legal, city1Enforcement
Perplexity system prompt expects: city1LegalScore, city1EnforcementScore
Result: Parser may fail to extract data

CRITICAL ISSUE #3: 100 Metrics = Truncation

Trying to get 100 metrics in one response causes truncation at ~40-60 metrics

The Severity Ranking Makes Sense:

ðŸ”´ P0: Scale conflict, key name conflict, 100-metric truncation
ðŸŸ¡ P1: Duplicate scale definitions (waste tokens)
ðŸŸ¢ P2: Duplicate evidence requirements (waste tokens)

The parseResponse() Function Is Doing Heroic Work:
It tries to handle:

Letter grades â†’ converts via letterToScore()
Category values â†’ converts via categoryToScore()
Numeric scores â†’ just clamps to 0-100

But it shouldn't have to guess which format it's receiving.

My Questions Before We Proceed:

Which scale do you want to standardize on?

A/B/C/D/E letters (then remove all 0-100 references)?
0-100 numeric (then remove all A-E references)?
Category values (then set USE_CATEGORY_SCORING=true and remove both)?


Which JSON key naming do you want?

city1Legal / city1Enforcement (current base prompt)
city1LegalScore / city1EnforcementScore (Perplexity system)


How do you want to handle 100 metrics?

Batch processing (multiple API calls, 20-25 metrics each)?
Keep single call but reduce total metrics?
Something else?



What's your direction?we are asking you which scale and what errors you see thats all we want you to study12:34 PMAnalyzed scale inconsistencies and identified key parsing errors.59sThe user is asking me to analyze and recommend which scale to use, and to identify what errors I see. They don't want me to ask them questions - they want my analysis and recommendation.
Let me think through this carefully:

What scale should be used?

Looking at the code flow:

parseResponse() at lines 119-180 handles conversion
It has logic for: letter grades, numeric scores, and category values
The FINAL output format is always 0-100 numeric scores
The letterToScore() function converts A-E to 0-100

Looking at what each LLM receives:

ALL addendums use 0-100
ALL system prompts use 0-100 (except Gemini which has none)
ONLY buildBasePrompt() uses A-E letters

The weight of evidence suggests 0-100 is the intended scale because:

It's used in 4/5 addendums
It's used in 4/5 system prompts
It's the final output format
It's more precise than 5-point letter scale


What errors do I see?

Let me trace through what happens when an LLM receives conflicting instructions:
Scenario 1: LLM follows buildBasePrompt (A-E)

Returns: {"city1Legal": "B", "city1Enforcement": "C"}
parseResponse() sees letter, calls letterToScore()
Converts Bâ†’75, Câ†’50
Works, but forces conversion

Scenario 2: LLM follows addendum/system (0-100)

Returns: {"city1LegalScore": 75, "city1EnforcementScore": 70}
parseResponse() looks for city1Legal - NOT FOUND
Falls back to checking for city1LegalScore - depends on parser logic
Looking at lines 144-180... I see it checks:

First: e.city1LegalCategory (category mode)
Then: e.city1Category (single category)
Then: Letter grade detection
Then: Numeric score detection



The parser has a fallback chain, but it's checking for the wrong field names. When an LLM returns city1LegalScore following the addendum instructions, the parser won't find city1Legal and will fail to extract the value properly. The parser expects city1Legal and city1Enforcement, but if the model outputs city1LegalScore instead, those fields remain undefined and default to 50. The validation then checks if the value is a letter (A-E) or number (0-100), converting accordinglyâ€”but with the wrong key name, this conversion never happens.
The system prompt creates multiple problems: it specifies the A-E scale initially, then switches to 0-100 multiple times, making it unclear which format the model should actually use. This redundancy wastes tokens and increases the chance of inconsistent output. Additionally, the A-E scale only provides 5 discrete values (100, 75, 50, 25, 0) compared to the full 0-100 range the parser can handle, losing precision in the scoring.
The solution is to standardize on the 0-100 numeric scale throughoutâ€”it's already what most prompts specify, it's the final output format anyway, and it eliminates the need for conversion logic. The key names should also be consistent (city1Legal not city1LegalScore), and all duplicate scale definitions should be removed from buildBasePrompt().RECOMMENDATION: Use 0-100 Numeric Scale

WHY 0-100 is the Correct Choice:
Evidence from the code:

Final output is 0-100 - The parser's ultimate goal is numeric scores (lines 119-180)
90% of prompts already use 0-100 - 4/5 addendums + 4/5 system prompts = majority
Better precision - 101 possible values (0-100) vs only 5 values (A-E)
No conversion overhead - Direct numeric output, no letterToScore() needed
Parser handles it natively - Lines 171-173 just clamp the number

Letter grades create problems:

Forces conversion step (letterToScore)
Only 5 granularity levels (A=100, B=75, C=50, D=25, E=0)
Conflicts with everything else in the system
Wastes tokens converting what the model could provide directly


ERRORS I SEE:
ðŸ”´ ERROR #1: Key Name Mismatch (CRITICAL)
Location: Perplexity system prompt (lines 1204-1237) vs parseResponse() (line 163)
The Problem:

Perplexity system says: Return city1LegalScore and city1EnforcementScore
Parser looks for: city1Legal and city1Enforcement (line 163)
Result: If Perplexity follows its system prompt, parser gets undefined for both values
Fallback: Default score of 50 used (line 132) - WRONG DATA

Code:
javascript// Line 163 - Parser expects these keys:
const legalValue = e.city1Legal || e.city2Legal;
const enforcementValue = e.city1Enforcement || e.city2Enforcement;

// Perplexity system prompt expects different keys:
// city1LegalScore, city1EnforcementScore
// â†’ Parser fails, returns default 50
```

---

### ðŸ”´ ERROR #2: Scale Format Confusion (CRITICAL)
**Location:** buildBasePrompt() vs all addendums/systems

**The Problem:**
- buildBasePrompt says: "Use A/B/C/D/E letter grades"
- Addendum says: "Use 0-100 numeric scale"
- System prompt says: "Use 0-100 numeric scale"
- **Result:** Model must choose which instruction to follow - unpredictable output format

**Example for GPT-4o:**
```
Layer 1 (buildBasePrompt): "Return city1Legal as A/B/C/D/E"
Layer 2 (gptAddendum): "Return scores as 0-100 numbers"
Layer 3 (system prompt): "Return scores as 0-100 numbers"
Model receives 1 instruction for letters, 2 for numbers â†’ which wins?

ðŸ”´ ERROR #3: Parser Assumes Format (CRITICAL)
Location: Lines 165-175
The Problem:
Parser uses type checking to guess format:
javascriptif (typeof legalValue === 'string' && /^[A-Ea-e]$/.test(legalValue)) {
  entry.legal = letterToScore(legalValue);
} else if (typeof legalValue === 'number') {
  entry.legal = Math.max(0, Math.min(100, legalValue));
}
What can go wrong:

Model returns "75" (string) instead of 75 (number) â†’ fails regex, no conversion
Model returns "B+" or "B-" â†’ fails regex, no conversion
Model returns letter but wrong key name â†’ undefined, gets default 50
No error thrown - just silent failure to default value


ðŸŸ¡ ERROR #4: Token Waste from Duplication (HIGH)
GPT-4o receives scale definition 3 times:

buildBasePrompt: Full A-E table (lines 231-298)
gptAddendum: Full 0-100 table (lines 758-776)
System prompt: Full 0-100 table (lines 797-816)

Perplexity receives scale definition 4 times:

buildBasePrompt: Full A-E table
perplexityAddendum: 0-100 instructions
System prompt: 0-100 table + JSON schema + evidence rules

Each table is ~500-1000 tokens â†’ wasting 1500-3000 tokens per call.

ðŸŸ¡ ERROR #5: Precision Loss (HIGH)
A-E Scale Granularity:

A = 100 (Excellent)
B = 75 (Good)
C = 50 (Moderate)
D = 25 (Poor)
E = 0 (Prohibited)

Problem: No way to express:

"Slightly better than moderate" (60)
"Not quite good" (70)
"Worse than moderate" (40)

Model forced into 5 buckets when system supports 101.

ðŸŸ¢ ERROR #6: Evidence Instructions Duplicated (MEDIUM)
Location: buildBasePrompt + every system prompt
Both define:

"Provide specific evidence"
"Use recent data"
"Cite sources"

Not breaking anything, but wastes tokens.

SUMMARY OF ACTUAL RUNTIME ERRORS:
When Perplexity is called:

Receives 4 conflicting instructions about scale/format
Likely follows system prompt â†’ returns city1LegalScore key
Parser looks for city1Legal â†’ gets undefined
Falls back to default score 50 for all metrics
Returns completely wrong data without error

When GPT-4o/Claude/Grok are called:

Receive 2-3 conflicting scale instructions
Model picks one format (unpredictable)
If picks letters: Extra conversion, loss of precision
If picks numbers but wrong key: Silent failure to default 50
Inconsistent behavior across models


THE FIX IS SIMPLE:

Remove all A-E letter grade instructions from buildBasePrompt()
Use 0-100 numeric scale uniformly
Standardize key names to city1Legal/city1Enforcement (no "Score" suffix)
Remove duplicate scale definitions (keep only in base prompt OR system, not both)
Add explicit validation instead of silent defaults